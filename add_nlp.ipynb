{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>building_id</th>\n",
       "      <th>created</th>\n",
       "      <th>description</th>\n",
       "      <th>display_address</th>\n",
       "      <th>features</th>\n",
       "      <th>interest_level</th>\n",
       "      <th>latitude</th>\n",
       "      <th>listing_id</th>\n",
       "      <th>longitude</th>\n",
       "      <th>manager_id</th>\n",
       "      <th>photos</th>\n",
       "      <th>price</th>\n",
       "      <th>street_address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>1.5</td>\n",
       "      <td>3</td>\n",
       "      <td>53a5b119ba8f7b61d4e010512e0dfc85</td>\n",
       "      <td>2016-06-24 07:54:24</td>\n",
       "      <td>A Brand New 3 Bedroom 1.5 bath ApartmentEnjoy ...</td>\n",
       "      <td>Metropolitan Avenue</td>\n",
       "      <td>[]</td>\n",
       "      <td>medium</td>\n",
       "      <td>40.7145</td>\n",
       "      <td>7211212</td>\n",
       "      <td>-73.9425</td>\n",
       "      <td>5ba989232d0489da1b5f2c45f6688adc</td>\n",
       "      <td>[https://photos.renthop.com/2/7211212_1ed4542e...</td>\n",
       "      <td>3000</td>\n",
       "      <td>792 Metropolitan Avenue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>c5c8a357cba207596b04d1afd1e4f130</td>\n",
       "      <td>2016-06-12 12:19:27</td>\n",
       "      <td></td>\n",
       "      <td>Columbus Avenue</td>\n",
       "      <td>[Doorman, Elevator, Fitness Center, Cats Allow...</td>\n",
       "      <td>low</td>\n",
       "      <td>40.7947</td>\n",
       "      <td>7150865</td>\n",
       "      <td>-73.9667</td>\n",
       "      <td>7533621a882f71e25173b27e3139d83d</td>\n",
       "      <td>[https://photos.renthop.com/2/7150865_be3306c5...</td>\n",
       "      <td>5465</td>\n",
       "      <td>808 Columbus Avenue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         bathrooms  bedrooms                       building_id  \\\n",
       "10.0           1.5         3  53a5b119ba8f7b61d4e010512e0dfc85   \n",
       "10000.0        1.0         2  c5c8a357cba207596b04d1afd1e4f130   \n",
       "\n",
       "                     created  \\\n",
       "10.0     2016-06-24 07:54:24   \n",
       "10000.0  2016-06-12 12:19:27   \n",
       "\n",
       "                                               description  \\\n",
       "10.0     A Brand New 3 Bedroom 1.5 bath ApartmentEnjoy ...   \n",
       "10000.0                                                      \n",
       "\n",
       "             display_address  \\\n",
       "10.0     Metropolitan Avenue   \n",
       "10000.0      Columbus Avenue   \n",
       "\n",
       "                                                  features interest_level  \\\n",
       "10.0                                                    []         medium   \n",
       "10000.0  [Doorman, Elevator, Fitness Center, Cats Allow...            low   \n",
       "\n",
       "         latitude  listing_id  longitude                        manager_id  \\\n",
       "10.0      40.7145     7211212   -73.9425  5ba989232d0489da1b5f2c45f6688adc   \n",
       "10000.0   40.7947     7150865   -73.9667  7533621a882f71e25173b27e3139d83d   \n",
       "\n",
       "                                                    photos  price  \\\n",
       "10.0     [https://photos.renthop.com/2/7211212_1ed4542e...   3000   \n",
       "10000.0  [https://photos.renthop.com/2/7150865_be3306c5...   5465   \n",
       "\n",
       "                  street_address  \n",
       "10.0     792 Metropolitan Avenue  \n",
       "10000.0      808 Columbus Avenue  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train = pd.read_json('input/train.json', orient='columns')\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- start with bath, beds, manager id, build id, price\n",
    "- transform manager id and building id into average interest level\n",
    "- standardize inputs\n",
    "- split into train/validation stratfied\n",
    "- take sample from both train and validation\n",
    "- train lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createSamples(X_train, y_train, X_test, y_test, num_train= 1000, num_test=400):\n",
    "    \n",
    "    nums = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(nums)\n",
    "    X_train_samp = X_train.iloc[nums[:num_train]]\n",
    "    y_train_samp = y_train.iloc[nums[:num_train]]\n",
    "\n",
    "\n",
    "    test_nums = np.arange(X_test.shape[0])\n",
    "    np.random.shuffle(test_nums)\n",
    "\n",
    "    X_test_samp = X_test.iloc[test_nums[:num_test]]\n",
    "    y_test_samp = y_test.iloc[test_nums[:num_test]]\n",
    "    return X_train_samp, y_train_samp, X_test_samp, y_test_samp\n",
    "\n",
    "def get_accuracy(preds, y_test_samp):\n",
    "    return (preds == y_test_samp).sum() / preds.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def subAndSample(subset, num_train=20000, num_test=2000, comp=False):\n",
    "    X = subset.drop('enc_interest', axis=1)\n",
    "    y = subset.enc_interest\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "    if comp:\n",
    "        return X_train, y_train, X_test, y_test \n",
    "    else:\n",
    "        return createSamples(X_train, y_train, X_test, y_test, num_train=num_train, num_test=num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def standardize(X_train, X_test):\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(X_train)\n",
    "    X_train = sc.transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['enc_interest'] = train.interest_level.map({'low': 0, 'medium': 1, 'high':2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## only baths, beds, price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset = train[['bathrooms', 'bedrooms', 'price', 'enc_interest']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = subAndSample(subset, comp=True)\n",
    "\n",
    "X_train, X_test = standardize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.29979\tval-merror:0.30078\n",
      "[1]\ttrain-merror:0.299055\tval-merror:0.300577\n",
      "[2]\ttrain-merror:0.299055\tval-merror:0.300577\n",
      "[3]\ttrain-merror:0.299005\tval-merror:0.300375\n",
      "[4]\ttrain-merror:0.298802\tval-merror:0.300172\n",
      "[5]\ttrain-merror:0.298549\tval-merror:0.300172\n",
      "[6]\ttrain-merror:0.29789\tval-merror:0.300071\n",
      "[7]\ttrain-merror:0.297738\tval-merror:0.300071\n",
      "[8]\ttrain-merror:0.297738\tval-merror:0.300071\n",
      "[9]\ttrain-merror:0.297612\tval-merror:0.29997\n",
      "[10]\ttrain-merror:0.297333\tval-merror:0.29997\n",
      "[11]\ttrain-merror:0.297333\tval-merror:0.29997\n",
      "[12]\ttrain-merror:0.297333\tval-merror:0.29997\n",
      "[13]\ttrain-merror:0.297333\tval-merror:0.29997\n",
      "[14]\ttrain-merror:0.297409\tval-merror:0.300172\n",
      "[15]\ttrain-merror:0.297384\tval-merror:0.300172\n",
      "[16]\ttrain-merror:0.297156\tval-merror:0.300577\n",
      "[17]\ttrain-merror:0.297181\tval-merror:0.300274\n",
      "[18]\ttrain-merror:0.297485\tval-merror:0.300274\n",
      "[19]\ttrain-merror:0.296801\tval-merror:0.30078\n",
      "[20]\ttrain-merror:0.297282\tval-merror:0.300679\n",
      "[21]\ttrain-merror:0.296674\tval-merror:0.300577\n",
      "[22]\ttrain-merror:0.29675\tval-merror:0.300274\n",
      "[23]\ttrain-merror:0.296522\tval-merror:0.300375\n",
      "[24]\ttrain-merror:0.296497\tval-merror:0.300375\n",
      "[25]\ttrain-merror:0.296421\tval-merror:0.300476\n",
      "[26]\ttrain-merror:0.296421\tval-merror:0.300476\n",
      "[27]\ttrain-merror:0.296396\tval-merror:0.300476\n",
      "[28]\ttrain-merror:0.29637\tval-merror:0.300476\n",
      "[29]\ttrain-merror:0.29637\tval-merror:0.300679\n",
      "[30]\ttrain-merror:0.296269\tval-merror:0.300983\n",
      "[31]\ttrain-merror:0.296294\tval-merror:0.300881\n",
      "[32]\ttrain-merror:0.295864\tval-merror:0.301084\n",
      "[33]\ttrain-merror:0.29556\tval-merror:0.30078\n",
      "[34]\ttrain-merror:0.295332\tval-merror:0.30078\n",
      "[35]\ttrain-merror:0.295053\tval-merror:0.300881\n",
      "[36]\ttrain-merror:0.294851\tval-merror:0.301388\n",
      "[37]\ttrain-merror:0.294597\tval-merror:0.301489\n",
      "[38]\ttrain-merror:0.294547\tval-merror:0.301489\n",
      "[39]\ttrain-merror:0.294623\tval-merror:0.301489\n",
      "[40]\ttrain-merror:0.294471\tval-merror:0.301287\n",
      "[41]\ttrain-merror:0.294243\tval-merror:0.301287\n",
      "[42]\ttrain-merror:0.294091\tval-merror:0.300375\n",
      "[43]\ttrain-merror:0.294116\tval-merror:0.301692\n",
      "[44]\ttrain-merror:0.294015\tval-merror:0.301591\n",
      "[45]\ttrain-merror:0.29404\tval-merror:0.301489\n",
      "[46]\ttrain-merror:0.293888\tval-merror:0.301489\n",
      "[47]\ttrain-merror:0.293888\tval-merror:0.301489\n",
      "[48]\ttrain-merror:0.293635\tval-merror:0.301591\n",
      "[49]\ttrain-merror:0.293686\tval-merror:0.301185\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label= y_train)\n",
    "dval = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# specify parameters via map\n",
    "watchlist = [(dtrain,'train'), (dval,'val')]\n",
    "param = {'max_depth':3, 'eta':1, 'silent':1, 'num_class': 3, 'objective':'multi:softprob' }\n",
    "num_round = 50\n",
    "bst = xgb.train(param, dtrain, num_round, watchlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.703851058787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6799519294229075"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = bst.predict(dval)\n",
    "print(log_loss(y_test, preds))\n",
    "log_loss(y_train, bst.predict(dtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] max_depth=2 .....................................................\n",
      "[CV] ............................ max_depth=2, score=0.688623 -  51.5s\n",
      "[CV] max_depth=2 .....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   51.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ max_depth=2, score=0.698198 -  49.8s\n",
      "[CV] max_depth=2 .....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ max_depth=2, score=0.692192 -  49.6s\n",
      "[CV] max_depth=4 .....................................................\n",
      "[CV] ............................ max_depth=4, score=0.687126 - 1.0min\n",
      "[CV] max_depth=4 .....................................................\n",
      "[CV] ............................ max_depth=4, score=0.707207 - 1.0min\n",
      "[CV] max_depth=4 .....................................................\n",
      "[CV] ............................ max_depth=4, score=0.686186 - 1.1min\n",
      "[CV] max_depth=6 .....................................................\n",
      "[CV] ............................ max_depth=6, score=0.685629 - 1.3min\n",
      "[CV] max_depth=6 .....................................................\n",
      "[CV] ............................ max_depth=6, score=0.695195 - 1.2min\n",
      "[CV] max_depth=6 .....................................................\n",
      "[CV] ............................ max_depth=6, score=0.675676 - 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  9.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='multiclass:softprob', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [2, 4, 6]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, scoring=None, verbose=3)"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train_concat, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.694357207983\n",
      "0.726617534243\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "preds = lr.predict(X_test)\n",
    "\n",
    "print(get_accuracy(preds, y_test))\n",
    "probs = lr.predict_proba(X_test)\n",
    "get_accuracy(lr.predict(X_train), y_train)\n",
    "print(log_loss(y_test, probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['feat_str'] = train.features.apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "train_counts = count_vect.fit_transform(train.feat_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(train_counts)\n",
    "train_tf = tf_transformer.transform(train_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TfidfTransformer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_tf, train.enc_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1070"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = clf.predict_proba(train_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7584492522367966"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(train.enc_interest, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6948046685038094"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(train.enc_interest, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.691723229663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.76581763436551764"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = train[['feat_str', 'enc_interest']]\n",
    "X_train, X_test, y_train, y_test = subAndSample(subset, comp=True)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB())])\n",
    "\n",
    "X_train_concat = np.array(np.concatenate([a for a in X_train.values]))\n",
    "X_test_concat = np.array(np.concatenate([a for a in X_test.values]))\n",
    "\n",
    "text_clf.fit(X_train_concat, y_train.values)\n",
    "preds = text_clf.predict(X_test_concat)\n",
    "probs = text_clf.predict_proba(X_test_concat)\n",
    "print(get_accuracy(y_test, preds))\n",
    "log_loss(y_test,probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.693344139398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.76033893198013003"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = train[['feat_str', 'enc_interest']]\n",
    "X_train, X_test, y_train, y_test = subAndSample(subset, comp=True)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                      ('clf', MultinomialNB())])\n",
    "\n",
    "X_train_concat = np.array(np.concatenate([a for a in X_train.values]))\n",
    "X_test_concat = np.array(np.concatenate([a for a in X_test.values]))\n",
    "\n",
    "text_clf.fit(X_train_concat, y_train.values)\n",
    "preds = text_clf.predict(X_test_concat)\n",
    "probs = text_clf.predict_proba(X_test_concat)\n",
    "print(get_accuracy(y_test, preds))\n",
    "log_loss(y_test,probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.694154594266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.76523884493207528"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = train[['feat_str', 'enc_interest']]\n",
    "X_train, X_test, y_train, y_test = subAndSample(subset, comp=True)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                      ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                      ('clf', MultinomialNB())])\n",
    "\n",
    "X_train_concat = np.array(np.concatenate([a for a in X_train.values]))\n",
    "X_test_concat = np.array(np.concatenate([a for a in X_test.values]))\n",
    "\n",
    "text_clf.fit(X_train_concat, y_train.values)\n",
    "preds = text_clf.predict(X_test_concat)\n",
    "probs = text_clf.predict_proba(X_test_concat)\n",
    "print(get_accuracy(y_test, preds))\n",
    "log_loss(y_test,probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.692128457097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.79231916508995459"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = train[['feat_str', 'enc_interest']]\n",
    "X_train, X_test, y_train, y_test = subAndSample(subset, comp=True)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range=(1, 2))),\n",
    "                      ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                      ('clf', MultinomialNB())])\n",
    "\n",
    "X_train_concat = np.array(np.concatenate([a for a in X_train.values]))\n",
    "X_test_concat = np.array(np.concatenate([a for a in X_test.values]))\n",
    "\n",
    "text_clf.fit(X_train_concat, y_train.values)\n",
    "preds = text_clf.predict(X_test_concat)\n",
    "probs = text_clf.predict_proba(X_test_concat)\n",
    "print(get_accuracy(y_test, preds))\n",
    "log_loss(y_test,probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.695674197143\n",
      "0.696486917758\n"
     ]
    }
   ],
   "source": [
    "subset = train[['feat_str', 'enc_interest']]\n",
    "X_train, X_test, y_train, y_test = subAndSample(subset, comp=True)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range=(1, 2))),\n",
    "                      ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                      ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                            alpha=1e-4, n_iter=10)) ])\n",
    "\n",
    "X_train_concat = np.array(np.concatenate([a for a in X_train.values]))\n",
    "X_test_concat = np.array(np.concatenate([a for a in X_test.values]))\n",
    "\n",
    "text_clf.fit(X_train_concat, y_train.values)\n",
    "preds = text_clf.predict(X_test_concat)\n",
    "train_preds = text_clf.predict(X_train_concat)\n",
    "\n",
    "print(get_accuracy(y_test, preds))\n",
    "print(get_accuracy(y_train, train_preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- split to train and test\n",
    "- fit svm on train\n",
    "- predict svm on test\n",
    "- add predictions as feature to classifier\n",
    "- get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset = train[['feat_str','bathrooms', 'bedrooms', 'price', 'enc_interest']]\n",
    "X_train, X_test, y_train, y_test = subAndSample(subset, comp=True)\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range=(1, 2))),\n",
    "                      ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                      ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                            alpha=1e-5, n_iter=5)) ])\n",
    "\n",
    "\n",
    "text_clf.fit(X_train.feat_str.values, y_train.values)\n",
    "\n",
    "preds = text_clf.predict(X_test.feat_str.values)\n",
    "\n",
    "train_preds = text_clf.predict(X_train.feat_str.values)\n",
    "\n",
    "X_test['svm_preds'] = preds\n",
    "X_train['svm_preds'] = train_preds\n",
    "\n",
    "X_train = X_train.drop('feat_str', axis=1)\n",
    "X_test= X_test.drop('feat_str', axis=1)\n",
    "\n",
    "X_train, X_test = standardize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "lr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69851078918\n",
      "0.722678949691\n"
     ]
    }
   ],
   "source": [
    "preds = lr.predict(X_test)\n",
    "probs = lr.predict_proba(X_test)\n",
    "print(get_accuracy(preds, y_test))\n",
    "\n",
    "\n",
    "print(log_loss(y_test, probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset = train[['feat_str','bathrooms', 'bedrooms', 'price', 'enc_interest']]\n",
    "X_train, X_test, y_train, y_test = subAndSample(subset, comp=True)\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range=(1, 2))),\n",
    "                      ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                      ('clf', MultinomialNB())])\n",
    "\n",
    "\n",
    "text_clf.fit(X_train.feat_str.values, y_train.values)\n",
    "\n",
    "preds = text_clf.predict(X_test.feat_str.values)\n",
    "\n",
    "train_preds = text_clf.predict(X_train.feat_str.values)\n",
    "\n",
    "X_test['svm_preds'] = preds\n",
    "X_train['svm_preds'] = train_preds\n",
    "\n",
    "X_train = X_train.drop('feat_str', axis=1)\n",
    "X_test= X_test.drop('feat_str', axis=1)\n",
    "\n",
    "X_train, X_test = standardize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "preds = lr.predict(X_test)\n",
    "\n",
    "print(get_accuracy(preds, y_test))\n",
    "\n",
    "get_accuracy(lr.predict(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.39618363e-01,  -4.36800222e-01,   5.95738072e-04,\n",
       "         -4.06530331e-01],\n",
       "       [  3.69696279e-02,   2.33803701e-03,   3.88956992e-05,\n",
       "          1.70429378e-01],\n",
       "       [  1.02648735e-01,   4.34462185e-01,  -6.34633771e-04,\n",
       "          2.36100953e-01]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "...                                            alpha=1e-3, n_iter=5, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate text and regular matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_proc = Pipeline([('vect', CountVectorizer(ngram_range=(1, 3))),\n",
    "                      ('tfidf', TfidfTransformer(use_idf=True)) ])\n",
    "\n",
    "\n",
    "subset = train[['feat_str','bathrooms', 'bedrooms', 'price', 'enc_interest']]\n",
    "X_train, X_test, y_train, y_test = subAndSample(subset, comp=True)\n",
    "\n",
    "text_proc.fit(X_train.feat_str.values)\n",
    "\n",
    "tr_text_mat = text_proc.transform(X_train.feat_str.values)\n",
    "\n",
    "test_text_mat = text_proc.transform(X_test.feat_str.values)\n",
    "\n",
    "X_train =X_train.drop('feat_str', axis=1)\n",
    "X_test =X_test.drop('feat_str', axis=1)\n",
    "X_train, X_test = standardize(X_train, X_test)\n",
    "\n",
    "X_train_concat = scipy.sparse.hstack((X_train,tr_text_mat))\n",
    "\n",
    "X_test_concat = scipy.sparse.hstack((X_test, test_text_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.706716644717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.71573668346799724"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "lr.fit(X_train_concat, y_train)\n",
    "\n",
    "preds = lr.predict(X_test_concat)\n",
    "\n",
    "print(get_accuracy(preds, y_test))\n",
    "\n",
    "get_accuracy(lr.predict(X_train_concat), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Xgboost with concatenated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_proc = Pipeline([('vect', CountVectorizer(ngram_range=(1, 3))),\n",
    "                      ('tfidf', TfidfTransformer(use_idf=True)) ])\n",
    "\n",
    "\n",
    "subset = train[['feat_str','bathrooms', 'bedrooms', 'price', 'enc_interest']]\n",
    "X_train, X_test, y_train, y_test = subAndSample(subset, comp=True)\n",
    "\n",
    "text_proc.fit(X_train.feat_str.values)\n",
    "\n",
    "tr_text_mat = text_proc.transform(X_train.feat_str.values)\n",
    "\n",
    "test_text_mat = text_proc.transform(X_test.feat_str.values)\n",
    "\n",
    "X_train =X_train.drop('feat_str', axis=1)\n",
    "X_test =X_test.drop('feat_str', axis=1)\n",
    "X_train, X_test = standardize(X_train, X_test)\n",
    "\n",
    "X_train_concat = scipy.sparse.hstack((X_train,tr_text_mat))\n",
    "\n",
    "X_test_concat = scipy.sparse.hstack((X_test, test_text_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.294344\tval-merror:0.293385\n",
      "[1]\ttrain-merror:0.292622\tval-merror:0.292574\n",
      "[2]\ttrain-merror:0.289405\tval-merror:0.291764\n",
      "[3]\ttrain-merror:0.288189\tval-merror:0.291257\n",
      "[4]\ttrain-merror:0.285606\tval-merror:0.291662\n",
      "[5]\ttrain-merror:0.282237\tval-merror:0.289535\n",
      "[6]\ttrain-merror:0.281224\tval-merror:0.288927\n",
      "[7]\ttrain-merror:0.279856\tval-merror:0.288623\n",
      "[8]\ttrain-merror:0.279046\tval-merror:0.288623\n",
      "[9]\ttrain-merror:0.278286\tval-merror:0.288319\n",
      "[10]\ttrain-merror:0.277222\tval-merror:0.286901\n",
      "[11]\ttrain-merror:0.275044\tval-merror:0.286496\n",
      "[12]\ttrain-merror:0.274107\tval-merror:0.2868\n",
      "[13]\ttrain-merror:0.273524\tval-merror:0.286394\n",
      "[14]\ttrain-merror:0.272967\tval-merror:0.285989\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train_concat, label= y_train)\n",
    "dval = xgb.DMatrix(X_test_concat, label=y_test)\n",
    "\n",
    "# specify parameters via map\n",
    "watchlist = [(dtrain,'train'), (dval,'val')]\n",
    "param = {'max_depth':3, 'eta':1, 'silent':1, 'num_class': 3, 'objective':'multi:softprob' }\n",
    "num_round = 15\n",
    "bst = xgb.train(param, dtrain, num_round, watchlist)\n",
    "\n",
    "preds = bst.predict(dval)\n",
    "\n",
    "print(log_loss(y_test, preds))\n",
    "\n",
    "log_loss(y_train, bst.predict(dtrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings that seem best so far:\n",
    "- use_idf =True\n",
    "- no stopwords\n",
    "- ngram range 2 or 3\n",
    "\n",
    "Xgboost on text features concatenated with regular features seems like the way to go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_proc = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "                      ('tfidf', TfidfTransformer(use_idf=True)) ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset = train[['description','bathrooms', 'bedrooms', 'price', 'enc_interest']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = subAndSample(subset, comp=True)\n",
    "\n",
    "text_proc.fit(X_train.description.values)\n",
    "\n",
    "tr_text_mat = text_proc.transform(X_train.description.values)\n",
    "\n",
    "test_text_mat = text_proc.transform(X_test.description.values)\n",
    "\n",
    "X_train =X_train.drop('description', axis=1)\n",
    "X_test =X_test.drop('description', axis=1)\n",
    "X_train, X_test = standardize(X_train, X_test)\n",
    "\n",
    "X_train_concat = scipy.sparse.hstack((X_train,tr_text_mat))\n",
    "\n",
    "X_test_concat = scipy.sparse.hstack((X_test, test_text_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.297688\tval-merror:0.294803\n",
      "[1]\ttrain-merror:0.295307\tval-merror:0.295715\n",
      "[2]\ttrain-merror:0.293584\tval-merror:0.29612\n",
      "[3]\ttrain-merror:0.28943\tval-merror:0.294803\n",
      "[4]\ttrain-merror:0.285758\tval-merror:0.293283\n",
      "[5]\ttrain-merror:0.284162\tval-merror:0.293081\n",
      "[6]\ttrain-merror:0.283174\tval-merror:0.29146\n",
      "[7]\ttrain-merror:0.279628\tval-merror:0.292676\n",
      "[8]\ttrain-merror:0.277551\tval-merror:0.290852\n",
      "[9]\ttrain-merror:0.275474\tval-merror:0.289535\n",
      "[10]\ttrain-merror:0.272764\tval-merror:0.288623\n",
      "[11]\ttrain-merror:0.2717\tval-merror:0.287813\n",
      "[12]\ttrain-merror:0.269978\tval-merror:0.28913\n",
      "[13]\ttrain-merror:0.268154\tval-merror:0.288826\n",
      "[14]\ttrain-merror:0.264887\tval-merror:0.287711\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train_concat, label= y_train)\n",
    "dval = xgb.DMatrix(X_test_concat, label=y_test)\n",
    "\n",
    "# specify parameters via map\n",
    "watchlist = [(dtrain,'train'), (dval,'val')]\n",
    "param = {'max_depth':3, 'eta':1, 'silent':1, 'num_class': 3, 'objective':'multi:softprob' }\n",
    "num_round = 15\n",
    "bst = xgb.train(param, dtrain, num_round, watchlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.664866422157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6263888617674026"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = bst.predict(dval)\n",
    "print(log_loss(y_test, preds))\n",
    "log_loss(y_train, bst.predict(dtrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_proc_feat = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "                      ('tfidf', TfidfTransformer(use_idf=True)) ])\n",
    "text_proc_desc = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "                      ('tfidf', TfidfTransformer(use_idf=True)) ])\n",
    "\n",
    "\n",
    "subset = train[['feat_str', 'description', 'bathrooms', 'bedrooms', 'price', 'enc_interest']]\n",
    "X_train, X_test, y_train, y_test = subAndSample(subset, comp=True)\n",
    "\n",
    "\n",
    "\n",
    "text_proc_feat.fit(X_train.feat_str.values)\n",
    "tr_feat_text = text_proc.transform(X_train.feat_str.values)\n",
    "test_feat_text = text_proc.transform(X_test.feat_str.values)\n",
    "\n",
    "\n",
    "text_proc_desc.fit(X_train.description.values)\n",
    "tr_desc_mat = text_proc_desc.transform(X_train.description.values)\n",
    "test_desc_mat = text_proc_desc.transform(X_test.description.values)\n",
    "\n",
    "\n",
    "X_train = X_train.drop(['feat_str', 'description'], axis=1)\n",
    "X_test = X_test.drop(['feat_str', 'description'], axis=1)\n",
    "X_train, X_test = standardize(X_train, X_test)\n",
    "\n",
    "\n",
    "X_train_concat = scipy.sparse.hstack((X_train,tr_feat_text, tr_desc_mat))\n",
    "X_test_concat = scipy.sparse.hstack((X_test, test_feat_text, test_desc_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.294952\tval-merror:0.294296\n",
      "[1]\ttrain-merror:0.293534\tval-merror:0.293283\n",
      "[2]\ttrain-merror:0.291431\tval-merror:0.291865\n",
      "[3]\ttrain-merror:0.286416\tval-merror:0.289738\n",
      "[4]\ttrain-merror:0.283655\tval-merror:0.287813\n",
      "[5]\ttrain-merror:0.276817\tval-merror:0.285179\n",
      "[6]\ttrain-merror:0.274157\tval-merror:0.28295\n",
      "[7]\ttrain-merror:0.27127\tval-merror:0.282646\n",
      "[8]\ttrain-merror:0.268965\tval-merror:0.282747\n",
      "[9]\ttrain-merror:0.26742\tval-merror:0.280215\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train_concat, label= y_train)\n",
    "dval = xgb.DMatrix(X_test_concat, label=y_test)\n",
    "\n",
    "# specify parameters via map\n",
    "watchlist = [(dtrain,'train'), (dval,'val')]\n",
    "param = {'max_depth':3, 'eta':1, 'silent':1, 'num_class': 3, 'objective':'multi:softprob' }\n",
    "num_round = 10\n",
    "bst = xgb.train(param, dtrain, num_round, watchlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.644885487823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6205642350065651"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = bst.predict(dval)\n",
    "print(log_loss(y_test, preds))\n",
    "log_loss(y_train, bst.predict(dtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.294952\tval-merror:0.294296\n",
      "[1]\ttrain-merror:0.293534\tval-merror:0.293283\n",
      "[2]\ttrain-merror:0.291431\tval-merror:0.291865\n",
      "[3]\ttrain-merror:0.286416\tval-merror:0.289738\n",
      "[4]\ttrain-merror:0.283655\tval-merror:0.287813\n",
      "[5]\ttrain-merror:0.276817\tval-merror:0.285179\n",
      "[6]\ttrain-merror:0.274157\tval-merror:0.28295\n",
      "[7]\ttrain-merror:0.27127\tval-merror:0.282646\n",
      "[8]\ttrain-merror:0.268965\tval-merror:0.282747\n",
      "[9]\ttrain-merror:0.26742\tval-merror:0.280215\n",
      "[10]\ttrain-merror:0.26514\tval-merror:0.280924\n",
      "[11]\ttrain-merror:0.263646\tval-merror:0.280417\n",
      "[12]\ttrain-merror:0.261645\tval-merror:0.280113\n",
      "[13]\ttrain-merror:0.259365\tval-merror:0.277378\n",
      "[14]\ttrain-merror:0.25744\tval-merror:0.276872\n"
     ]
    }
   ],
   "source": [
    "param = {'max_depth':3, 'eta':1, 'silent':1, 'num_class': 3, 'objective':'multi:softprob' }\n",
    "num_round = 15\n",
    "bst = xgb.train(param, dtrain, num_round, watchlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63915561909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6025585486170203"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = bst.predict(dval)\n",
    "print(log_loss(y_test, preds))\n",
    "log_loss(y_train, bst.predict(dtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.294952\tval-merror:0.294296\n",
      "[1]\ttrain-merror:0.293534\tval-merror:0.293283\n",
      "[2]\ttrain-merror:0.291431\tval-merror:0.291865\n",
      "[3]\ttrain-merror:0.286416\tval-merror:0.289738\n",
      "[4]\ttrain-merror:0.283655\tval-merror:0.287813\n",
      "[5]\ttrain-merror:0.276817\tval-merror:0.285179\n",
      "[6]\ttrain-merror:0.274157\tval-merror:0.28295\n",
      "[7]\ttrain-merror:0.27127\tval-merror:0.282646\n",
      "[8]\ttrain-merror:0.268965\tval-merror:0.282747\n",
      "[9]\ttrain-merror:0.26742\tval-merror:0.280215\n",
      "[10]\ttrain-merror:0.26514\tval-merror:0.280924\n",
      "[11]\ttrain-merror:0.263646\tval-merror:0.280417\n",
      "[12]\ttrain-merror:0.261645\tval-merror:0.280113\n",
      "[13]\ttrain-merror:0.259365\tval-merror:0.277378\n",
      "[14]\ttrain-merror:0.25744\tval-merror:0.276872\n",
      "[15]\ttrain-merror:0.256123\tval-merror:0.276264\n",
      "[16]\ttrain-merror:0.254831\tval-merror:0.274643\n",
      "[17]\ttrain-merror:0.252982\tval-merror:0.274339\n",
      "[18]\ttrain-merror:0.251159\tval-merror:0.274643\n",
      "[19]\ttrain-merror:0.249842\tval-merror:0.274643\n",
      "[20]\ttrain-merror:0.24893\tval-merror:0.274238\n",
      "[21]\ttrain-merror:0.246473\tval-merror:0.274643\n",
      "[22]\ttrain-merror:0.245232\tval-merror:0.275453\n",
      "[23]\ttrain-merror:0.244295\tval-merror:0.276163\n",
      "[24]\ttrain-merror:0.24351\tval-merror:0.276365\n"
     ]
    }
   ],
   "source": [
    "param = {'max_depth':3, 'eta':1, 'silent':1, 'num_class': 3, 'objective':'multi:softprob' }\n",
    "num_round = 25\n",
    "bst = xgb.train(param, dtrain, num_round, watchlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.634272339866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5737917812620611"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = bst.predict(dval)\n",
    "print(log_loss(y_test, preds))\n",
    "log_loss(y_train, bst.predict(dtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.294952\tval-merror:0.294296\n",
      "[1]\ttrain-merror:0.293534\tval-merror:0.293283\n",
      "[2]\ttrain-merror:0.291431\tval-merror:0.291865\n",
      "[3]\ttrain-merror:0.286416\tval-merror:0.289738\n",
      "[4]\ttrain-merror:0.283655\tval-merror:0.287813\n",
      "[5]\ttrain-merror:0.276817\tval-merror:0.285179\n",
      "[6]\ttrain-merror:0.274157\tval-merror:0.28295\n",
      "[7]\ttrain-merror:0.27127\tval-merror:0.282646\n",
      "[8]\ttrain-merror:0.268965\tval-merror:0.282747\n",
      "[9]\ttrain-merror:0.26742\tval-merror:0.280215\n",
      "[10]\ttrain-merror:0.26514\tval-merror:0.280924\n",
      "[11]\ttrain-merror:0.263646\tval-merror:0.280417\n",
      "[12]\ttrain-merror:0.261645\tval-merror:0.280113\n",
      "[13]\ttrain-merror:0.259365\tval-merror:0.277378\n",
      "[14]\ttrain-merror:0.25744\tval-merror:0.276872\n",
      "[15]\ttrain-merror:0.256123\tval-merror:0.276264\n",
      "[16]\ttrain-merror:0.254831\tval-merror:0.274643\n",
      "[17]\ttrain-merror:0.252982\tval-merror:0.274339\n",
      "[18]\ttrain-merror:0.251159\tval-merror:0.274643\n",
      "[19]\ttrain-merror:0.249842\tval-merror:0.274643\n",
      "[20]\ttrain-merror:0.24893\tval-merror:0.274238\n",
      "[21]\ttrain-merror:0.246473\tval-merror:0.274643\n",
      "[22]\ttrain-merror:0.245232\tval-merror:0.275453\n",
      "[23]\ttrain-merror:0.244295\tval-merror:0.276163\n",
      "[24]\ttrain-merror:0.24351\tval-merror:0.276365\n",
      "[25]\ttrain-merror:0.242344\tval-merror:0.276973\n",
      "[26]\ttrain-merror:0.240951\tval-merror:0.277986\n",
      "[27]\ttrain-merror:0.24004\tval-merror:0.277074\n",
      "[28]\ttrain-merror:0.237329\tval-merror:0.274643\n",
      "[29]\ttrain-merror:0.236468\tval-merror:0.273934\n",
      "[30]\ttrain-merror:0.235556\tval-merror:0.273427\n",
      "[31]\ttrain-merror:0.23505\tval-merror:0.273934\n",
      "[32]\ttrain-merror:0.233834\tval-merror:0.27444\n",
      "[33]\ttrain-merror:0.232365\tval-merror:0.276264\n",
      "[34]\ttrain-merror:0.231732\tval-merror:0.276568\n",
      "[35]\ttrain-merror:0.230795\tval-merror:0.277986\n",
      "[36]\ttrain-merror:0.22968\tval-merror:0.278493\n",
      "[37]\ttrain-merror:0.228718\tval-merror:0.27829\n",
      "[38]\ttrain-merror:0.227806\tval-merror:0.2791\n",
      "[39]\ttrain-merror:0.226767\tval-merror:0.278594\n",
      "[40]\ttrain-merror:0.226109\tval-merror:0.278999\n",
      "[41]\ttrain-merror:0.224842\tval-merror:0.279607\n",
      "[42]\ttrain-merror:0.223728\tval-merror:0.279708\n",
      "[43]\ttrain-merror:0.222968\tval-merror:0.2791\n",
      "[44]\ttrain-merror:0.222082\tval-merror:0.278493\n",
      "[45]\ttrain-merror:0.221195\tval-merror:0.278493\n",
      "[46]\ttrain-merror:0.219295\tval-merror:0.278493\n",
      "[47]\ttrain-merror:0.218561\tval-merror:0.279607\n",
      "[48]\ttrain-merror:0.217497\tval-merror:0.278493\n",
      "[49]\ttrain-merror:0.216737\tval-merror:0.278695\n",
      "[50]\ttrain-merror:0.215952\tval-merror:0.278999\n",
      "[51]\ttrain-merror:0.215217\tval-merror:0.27829\n",
      "[52]\ttrain-merror:0.214559\tval-merror:0.27829\n",
      "[53]\ttrain-merror:0.213495\tval-merror:0.278695\n",
      "[54]\ttrain-merror:0.213065\tval-merror:0.27829\n"
     ]
    }
   ],
   "source": [
    "param = {'max_depth':3, 'eta':1, 'silent':1, 'num_class': 3, 'objective':'multi:softprob' }\n",
    "num_round = 55\n",
    "bst = xgb.train(param, dtrain, num_round, watchlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.634050668408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.51333511762367889"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = bst.predict(dval)\n",
    "print(log_loss(y_test, preds))\n",
    "log_loss(y_train, bst.predict(dtrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Val Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgbc = XGBClassifier(objective='multiclass:softprob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgbc.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_proc_feat = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "                      ('tfidf', TfidfTransformer(use_idf=True)) ])\n",
    "text_proc_desc = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "                      ('tfidf', TfidfTransformer(use_idf=True)) ])\n",
    "\n",
    "\n",
    "subset = train[['feat_str', 'description', 'bathrooms', 'bedrooms', 'price', 'enc_interest']]\n",
    "X_train, y_train, X_test, y_test = subAndSample(subset, num_train=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_proc_feat.fit(X_train.feat_str.values)\n",
    "tr_feat_text = text_proc.transform(X_train.feat_str.values)\n",
    "\n",
    "\n",
    "\n",
    "text_proc_desc.fit(X_train.description.values)\n",
    "tr_desc_mat = text_proc_desc.transform(X_train.description.values)\n",
    "\n",
    "X_train = X_train.drop(['feat_str', 'description'], axis=1)\n",
    "\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "\n",
    "\n",
    "X_train_concat = scipy.sparse.hstack((X_train,tr_feat_text, tr_desc_mat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator=XGBClassifier(objective='multiclass:softprob'), param_grid=params, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] max_depth=2 .....................................................\n",
      "[CV] ............................ max_depth=2, score=0.688623 -  51.5s\n",
      "[CV] max_depth=2 .....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   51.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ max_depth=2, score=0.698198 -  49.8s\n",
      "[CV] max_depth=2 .....................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ max_depth=2, score=0.692192 -  49.6s\n",
      "[CV] max_depth=4 .....................................................\n",
      "[CV] ............................ max_depth=4, score=0.687126 - 1.0min\n",
      "[CV] max_depth=4 .....................................................\n",
      "[CV] ............................ max_depth=4, score=0.707207 - 1.0min\n",
      "[CV] max_depth=4 .....................................................\n",
      "[CV] ............................ max_depth=4, score=0.686186 - 1.1min\n",
      "[CV] max_depth=6 .....................................................\n",
      "[CV] ............................ max_depth=6, score=0.685629 - 1.3min\n",
      "[CV] max_depth=6 .....................................................\n",
      "[CV] ............................ max_depth=6, score=0.695195 - 1.2min\n",
      "[CV] max_depth=6 .....................................................\n",
      "[CV] ............................ max_depth=6, score=0.675676 - 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  9.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='multiclass:softprob', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [2, 4, 6]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, scoring=None, verbose=3)"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train_concat, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'max_depth':[6,8,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator=XGBClassifier(objective='multiclass:softprob'), param_grid=params, verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['address_lower'] = train.display_address.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8630"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.address_lower.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addr = train.groupby('address_lower').enc_interest.count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24450"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addr[:300].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addr_df = pd.DataFrame(np.zeros((train.shape[0],50)), index=train.index, columns = addr.index.values[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_ops = train.shape[0] * 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2467600"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for r,v in train.iterrows():\n",
    "    i+=1\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    for col in addr_df.columns:\n",
    "        if v[-1] == col:\n",
    "            addr_df.loc[r,col] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addr_sparse = scipy.sparse.csr_matrix(addr_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_concat = pd.concat([train, addr_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_concat = train.concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bathrooms', 'bedrooms', 'building_id', 'created', 'description',\n",
       "       'display_address', 'features', 'interest_level', 'latitude',\n",
       "       'listing_id', 'longitude', 'manager_id', 'photos', 'price',\n",
       "       'street_address', 'enc_interest', 'feat_str', 'address_lower'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns\n",
    "\n",
    "to_drop = ['building_id', 'created', 'description',\n",
    "       'display_address', 'features', 'interest_level', 'latitude',\n",
    "       'listing_id', 'longitude', 'manager_id', 'photos', 'street_address', 'feat_str', 'address_lower']\n",
    "\n",
    "train_concat = train_concat.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49352, 54)"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset= train[['bathrooms', 'bedrooms', 'price','enc_interest', 'address_lower']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = subAndSample(subset, comp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = FeatureHasher(input_type='string')\n",
    "\n",
    "hashed = h.fit_transform(X_train.address_lower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_hashed = h.transform(X_test.address_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.drop('address_lower', axis=1)\n",
    "X_test = X_test.drop('address_lower', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test = standardize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_concat = scipy.sparse.hstack((X_train,hashed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_concat = scipy.sparse.hstack((X_test, test_hashed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.299511\tval-merror:0.300071\n",
      "[1]\ttrain-merror:0.296649\tval-merror:0.299058\n",
      "[2]\ttrain-merror:0.296269\tval-merror:0.298551\n",
      "[3]\ttrain-merror:0.2948\tval-merror:0.296525\n",
      "[4]\ttrain-merror:0.294775\tval-merror:0.297032\n",
      "[5]\ttrain-merror:0.294116\tval-merror:0.296626\n",
      "[6]\ttrain-merror:0.294192\tval-merror:0.29612\n",
      "[7]\ttrain-merror:0.29328\tval-merror:0.296323\n",
      "[8]\ttrain-merror:0.293052\tval-merror:0.297842\n",
      "[9]\ttrain-merror:0.293255\tval-merror:0.298146\n",
      "[10]\ttrain-merror:0.292774\tval-merror:0.29845\n",
      "[11]\ttrain-merror:0.291786\tval-merror:0.29845\n",
      "[12]\ttrain-merror:0.291431\tval-merror:0.299159\n",
      "[13]\ttrain-merror:0.290798\tval-merror:0.298551\n",
      "[14]\ttrain-merror:0.290646\tval-merror:0.297842\n",
      "[15]\ttrain-merror:0.289658\tval-merror:0.298247\n",
      "[16]\ttrain-merror:0.289836\tval-merror:0.298247\n",
      "[17]\ttrain-merror:0.289506\tval-merror:0.298754\n",
      "[18]\ttrain-merror:0.288443\tval-merror:0.297842\n",
      "[19]\ttrain-merror:0.287759\tval-merror:0.298146\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train_concat, label= y_train)\n",
    "dval = xgb.DMatrix(X_test_concat, label=y_test)\n",
    "\n",
    "# specify parameters via map\n",
    "watchlist = [(dtrain,'train'), (dval,'val')]\n",
    "param = {'max_depth':3, 'eta':1, 'silent':1, 'num_class': 3, 'objective':'multi:softprob' }\n",
    "num_round = 20\n",
    "bst = xgb.train(param, dtrain, num_round, watchlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.689737723406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66608877414496059"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = bst.predict(dval)\n",
    "print(log_loss(y_test, preds))\n",
    "log_loss(y_train, bst.predict(dtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
